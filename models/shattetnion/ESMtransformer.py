# esm/esm/minimal_rope_transformer.py
# Minimal ESM-style Transformer encoder with Rotary Positional Embeddings (RoPE).
# Uses only modules you provided in esm/esm/modules.py.

from typing import Optional, Dict, Any, List
import torch
import torch.nn as nn

# åªå¼•ç”¨ä½ æä¾›è¿‡çš„æ¨¡å—
from esm.modules import (
    ESM1LayerNorm,
    TransformerLayer,
)


class MinimalRoPETransformer(nn.Module):
    """
    ä¸€ä¸ªæœ€å°çš„ ESM é£æ ¼ Transformer ç¼–ç å™¨ï¼Œä»…ä½¿ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ã€‚
    - ä¸åŒ…å« Learned/Sinusoidal ä½ç½®ç¼–ç 
    - ä¸åŒ…å« LM Head / Contact Head
    - æ”¯æŒ causal mask ä¸ padding mask
    - å±‚å†…å½’ä¸€åŒ–ä¸æ³¨æ„åŠ›/FFN å®ç°å®Œå…¨å¤ç”¨ä½ ç»™çš„ TransformerLayer

    Args:
        num_layers: Transformer å±‚æ•°
        embed_dim: é€šé“ç»´åº¦
        ffn_dim: FFN éšå±‚ç»´åº¦
        heads: æ³¨æ„åŠ›å¤´æ•°
        dropout: dropout æ¦‚ç‡ï¼ˆä½œç”¨äºå„å±‚å†…éƒ¨ï¼‰
        final_layer_norm: æ˜¯å¦åœ¨å †å ç»“æŸåå†åšä¸€æ¬¡ LayerNorm
    """

    def __init__(
        self,
        num_layers: int = 6,
        embed_dim: int = 512,
        ffn_dim: int = 2048,
        heads: int = 8,
        dropout: float = 0.1,
        final_layer_norm: bool = True,
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_layers = num_layers
        self.heads = heads

        # å †å  N ä¸ª ESM çš„ TransformerLayerï¼›
        # å…³é”®ï¼šuse_rotary_embeddings=Trueï¼Œä»…ç”¨ RoPEï¼Œä¸ä½¿ç”¨å¤–éƒ¨ä½ç½®åµŒå…¥
        self.layers = nn.ModuleList([
            TransformerLayer(
                embed_dim=embed_dim,
                ffn_embed_dim=ffn_dim,
                attention_heads=heads,
                add_bias_kv=True,
                use_esm1b_layer_norm=False,   # ä¸ç”¨ ESM1b fused LNï¼Œä¿æŒæœ€å°ä¾èµ–
                use_rotary_embeddings=True,   # ğŸ”‘ åªç”¨ RoPE
            )
            for _ in range(num_layers)
        ])

        self.final_ln = ESM1LayerNorm(embed_dim) if final_layer_norm else nn.Identity()

    @staticmethod
    def build_causal_mask(L: int, device: torch.device) -> torch.Tensor:
        """
        ç”Ÿæˆä¸Šä¸‰è§’ True çš„å¸ƒå°” maskï¼Œå½¢çŠ¶ [L, L]ã€‚
        True è¡¨ç¤ºä¸å¯è§ï¼ˆè¢« maskï¼‰ã€‚
        """
        return torch.triu(torch.ones(L, L, device=device, dtype=torch.bool), diagonal=1)

    def forward(
            self,
            x: torch.Tensor,
            *,
            causal: bool = False,
            padding_mask: Optional[torch.Tensor] = None,
            need_head_weights: bool = False,
    ) -> Dict[str, Any]:
        """
        x: [B, L, C]  -> å†…éƒ¨ä¼šè½¬æ¢ä¸º [L, B, C] ä»¥ç¬¦åˆ ESM çš„æ³¨æ„åŠ›å®ç°
        padding_mask: [B, L] (bool; True=mask)
        """
        B, L, C = x.shape
        assert C == self.embed_dim, f"Expected last dim {self.embed_dim}, got {C}"

        # å‡†å¤‡ mask
        if padding_mask is not None:
            # ç¡®ä¿å½¢çŠ¶ä¸ dtype
            assert padding_mask.shape == (B, L), f"padding_mask shape {padding_mask.shape} != {(B, L)}"
            padding_mask = padding_mask.to(dtype=torch.bool, device=x.device, non_blocking=True)
        attn_mask = self.build_causal_mask(L, x.device) if causal else None  # [L, L], bool

        # [B, L, C] -> [L, B, C]
        h = x.transpose(0, 1).contiguous()

        all_attn = []
        for layer in self.layers:
            # ESM çš„ MultiheadAttention æœŸæœ› [T, B, C] è¾“å…¥ï¼Œä»¥åŠ
            # attn_mask: [T, T], key_padding_mask: [B, T]
            h, attn = layer(
                h,
                self_attn_mask=attn_mask,
                self_attn_padding_mask=padding_mask,
                need_head_weights=need_head_weights,
            )
            if need_head_weights:
                # è§„èŒƒæˆ [B, H, L, L]
                if attn.dim() != 4:
                    raise ValueError(f"Unexpected attention rank: {attn.shape}")
                if attn.size(0) == B:
                    # [B, H, L, L]
                    attn_bhll = attn
                elif attn.size(1) == B:
                    # [H, B, L, L] -> [B, H, L, L]
                    attn_bhll = attn.permute(1, 0, 2, 3).contiguous()
                else:
                    # æœ‰äº›å®ç°ä¼šç»™ [B, L, L, H]
                    attn_bhll = attn.permute(0, 3, 1, 2).contiguous()
                all_attn.append(attn_bhll)

        # [L, B, C] -> [B, L, C]
        h = h.transpose(0, 1).contiguous()
        h = self.final_ln(h)

        out: Dict[str, Any] = {"hidden_states": h}
        if need_head_weights and len(all_attn) > 0:
            # å †å åˆ° [B, num_layers, H, L, L]
            out["attentions"] = torch.stack(all_attn, dim=1)
        return out


# -----------------------------
# ç®€å•ç”¨ä¾‹ï¼ˆç¤ºèŒƒï¼‰
# -----------------------------
if __name__ == "__main__":
    B, L, C = 2, 128, 512
    model = MinimalRoPETransformer(
        num_layers=6,
        embed_dim=C,
        ffn_dim=2048,
        heads=8,
        dropout=0.1,
        final_layer_norm=True,
    )

    # ä½ è‡ªå·±çš„å¤–éƒ¨åµŒå…¥ï¼šä¾‹å¦‚ token embedding æˆ–çº¿æ€§æŠ•å½±åˆ° C ç»´
    x = torch.randn(B, L, C)

    # padding maskï¼šTrue=maskï¼ˆä¾‹å¦‚å 10 ä¸ªä¸º padï¼‰
    pad = torch.zeros(B, L, dtype=torch.bool)
    pad[:, -10:] = True

    out = model(x, causal=False, padding_mask=pad, need_head_weights=True)
    print(out["hidden_states"].shape)   # [B, L, C]
    print(out["attentions"].shape)      # [B, num_layers, heads, L, L]
