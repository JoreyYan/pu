# 2026-02-21 20:52:02 +0800 - OpenAI - IGA 12D Enable

## Goal
Enable 12D ellipsoid training (backbone + ellipsoid params) now that IGA backbone-only is stable and competitive.

## Changes
- Enable gaussian param corruption in interpolant: `interpolant.gaussian_params.corrupt: true`.
- Switch FlowModelIGA update from BackboneUpdate (6D) to GaussianUpdateBlock (12D).
- Return ellipsoid predictions from FlowModelIGA: `ellipsoid_alpha`, `ellipsoid_scaling_log`.
- Add ellipsoid losses in FlowModule:
  - local_mean MSE (from alpha * exp(scaling_log)), time-normalized.
  - scaling_log MSE, time-normalized.
- Store `scaling_log_1` in noisy_batch for loss.
- Update W&B name/notes to reflect 12D + G-noise + ellipsoid losses.

## Rationale
Backbone-only is not ellipsoid's strongest test. The 12D path is the first task where ellipsoid parameters are explicitly supervised, so we can evaluate whether ellipsoid representation provides practical advantage.

## Files touched
- `data/interpolant.py` (store scaling_log_1; enable gaussian param corruption via config)
- `models/flow_model.py` (GaussianUpdateBlock in FlowModelIGA; return ellipsoid preds)
- `models/flow_module.py` (ellipsoid losses)
- `configs/Train_fm.yaml` (gaussian_params.corrupt + loss weights + W&B name/notes)

## Expected checks
- Training runs without DDP unused-parameter errors.
- New loss keys appear:
  - `train/ellipsoid_local_mean_loss`
  - `train/ellipsoid_scaling_loss`
- Backbone losses (trans/rot) remain in the same ballpark as vuzldfte early on.
- Ellipsoid losses decrease over steps.

## Next actions
- Run a short sanity training (1-2k steps) and compare to IPA baseline on the same window.
- If ellipsoid losses are flat or explode: inspect `debug/iga/*` and consider adjusting corrupt schedule or loss weights.
