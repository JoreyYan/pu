# 2026-02-21 22:00:00 +0800 — Claude — Decouple IGA attention geometry from corrupted ellipsoid params

## Context

- Baseline run: 12D run from `2026-02-21_20-52-02_+0800_openai_iga_12d_enable.md` (commit `ce1212e`)
- Previous 6D reference: `vuzldfte` (IGA backbone-only, stable)
- Current model/config: FlowModelIGA with GaussianUpdateBlock (12D), `gaussian_params.corrupt: true`
- Problem: Total `se3_vf_loss` regressed compared to 6D. Backbone losses (trans=1.19, rot=1.40 at step 1047) are actually comparable, but IGA attention quality is degraded by corrupted ellipsoid geometry.

## Change

### Files touched
- `models/flow_model.py` — `FlowModelIGA.forward()` (+ `import math`)
- `configs/Train_fm.yaml` — wandb name/notes

### What changed (high level)
Decoupled the geometry used for IGA attention from the geometry being denoised. IGA attention now uses **GT (clean) ellipsoid params** (`scaling_log_1`, `local_mean_1`) for computing Gaussian overlap, while the GaussianUpdateBlock still operates on and denoises the **corrupted** `rigids_nm`.

### Key code paths

**1. GT geometry extraction** (after `rigids_nm = self._make_rigids_nm(input_feats)`, ~line 1133):

```python
if 'scaling_log_1' in input_feats and 'local_mean_1' in input_feats:
    gt_scaling_log_nm = input_feats['scaling_log_1'] + math.log(du.ANG_TO_NM_SCALE)
    gt_local_mean_nm = input_feats['local_mean_1'] * du.ANG_TO_NM_SCALE
    iga_rigids_nm = OffsetGaussianRigid(
        rigids_nm._rots, rigids_nm._trans,
        gt_scaling_log_nm, gt_local_mean_nm,
    )
else:
    iga_rigids_nm = rigids_nm  # fallback for inference
```

**2. Block loop** — IGA receives `iga_rigids_nm`, update block receives `rigids_nm`:

```python
iga_out = iga_mod(s=node_embed, z=edge_embed, r=iga_rigids_nm, mask=node_mask)
...
rigids_nm = self.trunk[f'gauss_update_{b}'](
    node_embed, rigids_nm, mask=(node_mask * diffuse_mask)
)
```

**3. Sync backbone after update** — keep GT ellipsoid but track updated trans/rot:

```python
if iga_rigids_nm is not rigids_nm:
    iga_rigids_nm = OffsetGaussianRigid(
        rigids_nm._rots, rigids_nm._trans,
        iga_rigids_nm._scaling_log, iga_rigids_nm._local_mean,
    )
```

## Rationale / logic

### Root cause (two factors)

**Factor 1 (main): Corrupted geometry degrades IGA attention.**
In 6D mode, `rigids_t._local_mean` and `rigids_t._scaling_log` were always GT, so `transform_to_global_gaussian()` had accurate anchors. In 12D mode with `gaussian_params.corrupt: true`, these are linearly interpolated with noise via `_corrupt_parameter()`. At low t (near t=0), they are nearly pure noise → Gaussian overlap distances are meaningless → geometry branch gives random bias → attention quality degrades → backbone prediction also suffers.

**Factor 2: Inflated total loss.**
`se3_vf_loss = trans + rot + aux + 1.0*local_mean_loss + 1.0*scaling_loss`. The new ellipsoid terms (3.49 + 1.82 = 5.31 at step 1047) inflate the total, but backbone-specific losses are actually comparable to 6D.

### Hypothesis
Providing clean ellipsoid geometry to IGA will restore attention quality to 6D-comparable levels, while still allowing the model to learn ellipsoid denoising through the update block and loss terms.

### Expected effect
- `train/trans_loss` and `train/rots_vf_loss` should improve (or at least match 6D `vuzldfte`)
- `train/ellipsoid_local_mean_loss` and `train/ellipsoid_scaling_loss` should still decrease (update block still denoises)
- IGA debug metrics (`geo_scaled_absmax`, `wmax_sat_frac`) should be more stable

### Risks
- **Train-inference gap**: At inference time, GT is unavailable. The `else` branch falls back to using the model's own (corrupted/predicted) geometry. If the model relies too heavily on clean geometry during training, inference quality could suffer.
- **Mitigation**: After the model learns reasonable ellipsoid predictions, consider a curriculum: switch to using predicted geometry for attention (or blend GT/predicted).

## How to run / compare

- W&B name: `hallucination_pdb_IGAattn_GaussUpdate_12D_Gnoise_gtGeoAttn_logdetClamp20_sigmaMin03nm_scaleP_geoInitM4`
- Key metrics to monitor:
  - `train/trans_loss`, `train/rots_vf_loss` — should match or beat 6D levels
  - `train/ellipsoid_local_mean_loss`, `train/ellipsoid_scaling_loss` — should decrease
  - `train/se3_vf_loss` — total (will still include ellipsoid terms)
- Debug signals: `debug/iga/global/geo_scaled_absmax`, `debug/iga/global/wmax_sat_frac`, `debug/iga/global/log_det_min`

## Results

*(pending — fill in after training run)*

## Next steps

- **If backbone losses match/beat 6D and ellipsoid losses decrease**: Success. Consider reducing ellipsoid loss weights from 1.0 → 0.25 if they still dominate total loss.
- **If backbone regresses despite GT geometry**: The issue is elsewhere (e.g., GaussianUpdateBlock interaction, gradient competition). Try zeroing ellipsoid loss weights to isolate.
- **Long-term**: Curriculum from GT → predicted geometry for attention. Once ellipsoid predictions are good, remove GT dependency so train matches inference.
