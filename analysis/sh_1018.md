  # 当前方案：
  扩散(atom14坐标, 序列) -> 联合去噪 -> (atom14, sequence)
                                      ↓
                                ProteinMPNN/ESM

  # 你想要的方案：
  扩散(SH密度, 序列) -> 联合去噪 -> (SH density, sequence)
                                      ↓
                                SH-VAE/SH-LM (新的预训练模型!)

  核心问题：SH密度 + 序列的联合扩散，能否产生更"化学合理"的序列？

  ---
  1️⃣ SH密度扩散能否得到更好的序列？

  我的答案：非常有可能YES！理由如下

  理由1: 结构-序列耦合更紧密

  # R3坐标扩散的问题：
  # - 坐标和序列是"松耦合"的
  #   atom14[n] = [14, 3]  # 只有几何
  #   seq[n] = 'F'          # 只有类型
  # - 模型需要学习："这个几何形状对应PHE"

  # SH密度扩散：
  # - 密度和序列是"强耦合"的
  #   density[n, C=0] = 碳密度 -> 直接限制可能的氨基酸
  #   density[n, C=1] = 氮密度 -> 进一步限制
  #   density[n, C=3] = 硫密度 -> 只能是CYS/MET
  # - 密度本身就是"带元素标签的几何"

  具体例子：

  # Case 1: 芳香环氨基酸
  # R3: 看到6个C排成环 -> 可能是PHE, TYR, TRP, HIS
  # SH: 
  #   - C密度显示环状 (L=2,4分量)
  #   - N密度在某位置 -> 排除PHE
  #   - O密度在末端 -> 确定是TYR
  # -> SH密度包含更多化学信息！

  # Case 2: 含硫氨基酸
  # R3: 看到一个远离主链的原子 -> 不知道是什么
  # SH: 
  #   - S密度峰 -> 只能是CYS或MET
  #   - 距离CA的r -> 判断是哪个
  # -> 序列预测更确定！

  理由2: 化学约束隐式编码

  # SH密度训练时学到的"先验"：
  # 1. 密度守恒：
  #    sum(density[n, C]) ≈ 该氨基酸的C原子数
  #    sum(density[n, N]) ≈ 该氨基酸的N原子数

  # 2. 密度分布模式：
  #    - 芳香环：环状π电子云
  #    - 脂肪链：线性密度
  #    - 带电基团：局部高密度

  # 3. 元素相容性：
  #    - 如果有S密度 -> seq必须是C/M
  #    - 如果无侧链密度 -> seq必须是G
  #    - 如果有多个N -> seq可能是R/K/H

  # 这些约束在R3坐标中不存在！

  理由3: 去噪路径更"化学"

  # R3扩散的去噪：
  # t=1.0: 随机噪声
  # t=0.5: 模糊的原子团
  # t=0.0: 精确的原子位置
  # -> 中间步没有明确的化学意义

  # SH密度扩散：
  # t=1.0: 随机密度场
  # t=0.5: 模糊的密度云（但密度守恒！）
  # t=0.0: 清晰的原子密度峰
  # -> 每一步都可以解释为"部分确定的电子密度"
  # -> 序列预测在每一步都可以基于"化学环境"

  ---
  2️⃣ SH密度预训练模型的可行性

  这是一个非常创新的想法！让我详细分析：

  为什么ESM这么强？

  # ESM (Evolutionary Scale Modeling):
  # 1. 预训练：在2.5亿蛋白质序列上做masked language modeling
  #    输入: "M-K-[MASK]-I-F-..."
  #    输出: 预测[MASK]位置的氨基酸
  # 2. 学到：
  #    - 氨基酸共现模式
  #    - 进化保守性
  #    - 结构motif (间接)

  # ESM的局限：
  # ❌ 只看序列，不直接看结构
  # ❌ 无法编码精细的几何信息
  # ❌ 无法表示"柔性/刚性"

  SH密度预训练模型的优势

  # SH-VAE / SH-LM (你的提案):
  # 预训练：在大量蛋白质结构上
  #    输入: SH_density[n-2:n+2]  # 上下文残基的密度
  #    输出: 预测中心残基的序列 或 完整密度

  # 能学到什么？
  # ✅ 结构-序列直接关系（不是间接）
  # ✅ 化学环境（元素分布）
  # ✅ 几何约束（键长、键角、平面性）
  # ✅ 柔性/刚性区域
  # ✅ 相互作用模式（π-π堆积、盐桥）

  具体架构建议

  方案A: SH-VAE (变分自编码器)

  class SH_VAE(nn.Module):
      def __init__(self, C=4, L_max=8, R_bins=16):
          self.encoder = SHEncoder(C, L_max, R_bins, latent_dim=256)
          self.decoder = SHDecoder(latent_dim=256, C, L_max, R_bins)

      def forward(self, sh_density):
          # sh_density: [B, N, C, L+1, 2L+1, R]

          # Encode to latent
          mu, logvar = self.encoder(sh_density)  # [B, N, latent_dim]
          z = reparameterize(mu, logvar)

          # Decode back
          sh_recon = self.decoder(z)  # [B, N, C, L+1, 2L+1, R]

          return sh_recon, mu, logvar

  # 预训练任务：
  # 1. 重建：VAE loss = reconstruction + KL
  # 2. Masked prediction：mask某些残基的密度，预测它
  # 3. 序列条件：给定序列，生成密度（类似structure prediction）

  预训练数据：
  - PDB + AlphaFold DB：~2亿蛋白质结构
  - 转换为SH密度（离线计算，存储）

  方案B: SH-LM (语言模型风格)

  class SH_LanguageModel(nn.Module):
      def __init__(self):
          self.sh_embedding = SHFeatureHead(...)  # 你已有的
          self.transformer = nn.TransformerEncoder(...)
          self.seq_head = nn.Linear(hidden, 20)  # 预测氨基酸
          self.density_head = SHPredictionHead(...)  # 预测密度

      def forward(self, sh_density, mask_positions):
          # Embed SH to features
          feat = self.sh_embedding(sh_density)  # [B,N,hidden]

          # Mask some positions
          feat[mask_positions] = self.mask_token

          # Transformer
          feat = self.transformer(feat)  # [B,N,hidden]

          # Predict masked residues
          seq_logits = self.seq_head(feat[mask_positions])
          sh_pred = self.density_head(feat[mask_positions])

          return seq_logits, sh_pred

  # 预训练任务：类似BERT
  # 1. Mask 15%的残基
  # 2. 预测：序列 + 密度

  方案C: 对比学习 (CLIP风格)

  class SH_CLIP(nn.Module):
      def __init__(self):
          self.sh_encoder = SHEncoder(...)
          self.seq_encoder = ESM_Encoder(...)  # 借用ESM

      def forward(self, sh_density, sequence):
          # Encode both modalities
          sh_emb = self.sh_encoder(sh_density)    # [B,N,D]
          seq_emb = self.seq_encoder(sequence)    # [B,N,D]

          # Contrastive loss
          loss = clip_loss(sh_emb, seq_emb)
          return loss

  # 预训练：让SH密度和序列的embedding空间对齐
  # 好处：可以用ESM的预训练知识！

  ---
  3️⃣ 与ESM对比：SH预训练的独特优势

  | 维度    | ESM (序列) | SH-LM (密度) | 优势     |
  |-------|----------|------------|--------|
  | 输入    | 氨基酸序列    | 3D密度场      | SH包含几何 |
  | 化学信息  | 无        | 元素类型       | SH明确   |
  | 几何约束  | 无        | 键长/键角      | SH内建   |
  | 柔性表示  | 无        | 密度分布宽度     | SH天然   |
  | 预训练数据 | 2.5亿序列   | ~2亿结构      | ESM更多  |
  | 计算效率  | 高        | 中-低        | ESM更快  |

  关键insight：
  # ESM学习：P(seq | evolutionary_context)
  # SH-LM学习：P(seq | geometric_&_chemical_context)

  # 两者互补！
  # 最优方案：融合两者
  seq_logits = α * ESM(seq) + β * SH_LM(density)

  ---
  4️⃣ 实施路线图

  Phase 1: 验证概念 (2-4周)

  # 1.1 小规模SH-VAE
  # - 数据：1000个蛋白质
  # - 任务：重建SH密度
  # - 评估：重建误差，decode的atom14 RMSD

  # 1.2 序列-密度耦合测试
  # - 给定SH密度，预测序列
  # - 对比：SH->seq vs atom14->seq
  # - 评估：序列准确率

  # 1.3 化学合理性
  # - SH密度是否满足元素守恒？
  # - 预测的序列-密度是否一致？

  Phase 2: 扩大规模 (1-2个月)

  # 2.1 大规模预训练
  # - 数据：PDB (~20万) + AlphaFold DB (筛选高质量的100万)
  # - 模型：SH-Transformer (类似ESM架构)
  # - 任务：Masked SH prediction

  # 2.2 下游任务微调
  # - Structure prediction
  # - Inverse folding (你的任务)
  # - Protein design

  Phase 3: 与ESM融合 (2-3个月)

  # 3.1 双模态预训练
  # - 同时用序列和SH密度
  # - 学习联合表示

  # 3.2 应用到你的扩散模型
  # - 用预训练的SH-LM初始化encoder
  # - 扩散：(SH, seq) -> 去噪 -> (SH, seq)
  # - 评估：sequence quality, structure quality

  ---
  5️⃣ 技术挑战

  挑战1: 计算成本

  # SH密度维度：
  C * (L+1) * (2L+1) * R_bins = 4 * 9 * 17 * 16 = 9,792 per residue

  # 对比：
  ESM输入：20 (one-hot) per residue

  # 解决方案：
  # - 压缩：用低秩分解、量化
  # - 稀疏：只保留significant的L,M分量
  # - 混合精度：bf16训练

  挑战2: 数据规模

  # ESM: 2.5亿序列（来自进化数据库）
  # SH: 最多~2亿结构（PDB+AlphaFold DB）

  # 但质量更高：
  # - 每个结构包含几何信息
  # - 可以数据增强（旋转、小扰动）

  挑战3: 评估指标

  # 如何评估SH预训练模型的质量？
  # 1. 重建精度（RMSD）
  # 2. 序列预测准确率
  # 3. 下游任务性能（inverse folding, design）
  # 4. 化学合理性（新指标！）

  ---
  🎯 我的最终建议

  立即开始的实验（优先级排序）

  | 优先级   | 实验                   | 时间  | 价值     |
  |-------|----------------------|-----|--------|
  | ⭐⭐⭐⭐⭐ | SH密度->序列 vs R3->序列对比 | 1周  | 验证基本假设 |
  | ⭐⭐⭐⭐⭐ | 小规模SH-VAE (1k蛋白质)    | 2周  | 验证可行性  |
  | ⭐⭐⭐⭐  | SH+序列联合扩散            | 3周  | 你的核心想法 |
  | ⭐⭐⭐⭐  | 化学合理性指标设计            | 1周  | 评估基础   |
  | ⭐⭐⭐   | 大规模SH预训练             | 2月  | 长期方向   |

  最激动人心的方向

  你的想法**"用SH密度预训练模型代替ESM"**是非常有前景的！

  原因：
  1. ✅ SH包含ESM无法获取的几何+化学信息
  2. ✅ 结构-序列耦合更紧密
  3. ✅ 可以与ESM互补甚至超越（在结构相关任务上）

  建议：
  - 先做小规模验证（Phase 1）
  - 如果promising，可以发一篇方法论文
  - 然后做大规模预训练（可能需要更多GPU资源）

  ---
