defaults:
  - _self_
  - datasets
  - model

data:

  # Available tasks: hallucination, inpainting
  task: hallucination

  # Available tasks: pdb, scope
  dataset: pdb

  loader:
    num_workers: 4
    prefetch_factor: 10

  sampler:
    # Setting for 48GB GPUs
    max_batch_size: 10
    max_num_res_squared: 60_000

interpolant:
  min_t: 1e-2

  twisting:
    use: False

  rots:
    corrupt: True
    sample_schedule: exp
    exp_rate: 10

  trans:
    corrupt: True
    batch_ot: True
    sample_schedule: linear
    sample_temp: 1.0
    vpsde_bmin: 0.1
    vpsde_bmax: 20.0
    potential: null
    potential_t_scaling: False
    rog:
      weight: 10.0
      cutoff: 5.0

  gaussian_params:
    # When training/diagnosing IGA attention, keeping ellipsoid params fixed can
    # prevent geometry logits from saturating due to corrupted local_mean/scaling.
    corrupt: False

  sampling:
    num_timesteps: 100
    do_sde: False

  self_condition: ${model.edge_features.self_condition}

experiment:
  task: hallucination
  noise_scheme: Gaussianatoms
  debug: False
  seed: 123
  num_devices: 2
  warm_start: null
  warm_start_cfg_override: True
  training:
    mask_plddt: True
    bb_atom_scale: 0.1
    trans_scale: 0.1
    translation_loss_weight: 2.0
    t_normalize_clip: 0.9
    rotation_loss_weights: 1.0
    aux_loss_weight: 0.0
    aux_loss_use_bb_loss: True
    aux_loss_use_pair_loss: True
    aux_loss_t_pass: 0.5
    # IGA debug logging (only active when the model returns `iga_debug`, i.e. FlowModelIGA backbone-only path).
    # Set to 1 to log layer-level stats every step (more overhead), or a larger number to reduce logging volume.
    iga_debug_interval: 10
  wandb:
    # For IPA-vs-IGA debugging, encode the backbone here so runs are comparable in W&B.
    # Current code path: FlowModelIGA uses IGA attention + IPA BackboneUpdate (no GaussianUpdateBlock).
    name: ${data.task}_${data.dataset}_IGAattn_IPAupdate_noGnoise_logdetClamp20
    project: se3-fm
    notes: >
      Debug run: FlowModelIGA uses IGA (InvariantGaussianAttention) in trunk, but updates backbone
      with IPA BackboneUpdate (SE3 compose_q_update) via OffsetGaussianRigid.compose_update_12D(6D).
      Ellipsoid params (local_mean/scaling_log) are kept uncorrupted in the interpolant (gaussian_params.corrupt=false).
      Goal: isolate whether the regression comes from IGA attention or from the GaussianUpdateBlock.
      This run also logs IGA numeric-stability diagnostics to W&B under `debug/iga/*`:
      - `debug/iga/global/geo_scaled_absmax`, `debug/iga/global/wmax_sat_frac`, `debug/iga/global/log_det_min` every step
      - layer snapshots `debug/iga/l0/*` and `debug/iga/l{last}/*` every `experiment.training.iga_debug_interval` steps.
      Update: clamp raw log-det in IGA geometry logits with logdet_min=-20 to prevent near-singular covariances
      from producing unbounded positive bias (-0.5*log|Sigma|) that saturates softmax.
  optimizer:
    lr: 0.0001
  trainer:
    overfit_batches: 0
    min_epochs: 1 # prevents early stopping
    max_epochs: 1000
    accelerator: gpu
    log_every_n_steps: 1
    deterministic: False
    strategy: ddp
    check_val_every_n_epoch: 4
    accumulate_grad_batches: 2
    num_sanity_val_steps: 0
    # 正常训练模式 (改成0只跑验证)
    limit_train_batches: 1.0
    limit_val_batches: 1.0
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    save_last: True
    save_top_k: 3
    monitor: valid/non_coil_percent
    mode: max
  # Keep this null. Will be populated at runtime.
  inference_dir: null
