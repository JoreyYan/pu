defaults:
  - datasets
  - model
  - _self_

data:

  # Available tasks: hallucination, inpainting
  task: hallucination

  # Available tasks: pdb, scope
  dataset: pdb

  loader:
    num_workers: 4
    prefetch_factor: 10

  sampler:
    # Setting for 48GB GPUs
    max_batch_size: 10
    max_num_res_squared: 60_000

interpolant:
  min_t: 1e-2

  twisting:
    use: False

  rots:
    corrupt: True
    sample_schedule: exp
    exp_rate: 10

  trans:
    corrupt: True
    batch_ot: True
    sample_schedule: linear
    sample_temp: 1.0
    vpsde_bmin: 0.1
    vpsde_bmax: 20.0
    potential: null
    potential_t_scaling: False
    rog:
      weight: 10.0
      cutoff: 5.0

  gaussian_params:
    # When training/diagnosing IGA attention, keeping ellipsoid params fixed can
    # prevent geometry logits from saturating due to corrupted local_mean/scaling.
    corrupt: True

  sampling:
    num_timesteps: 100
    do_sde: False

  self_condition: ${model.edge_features.self_condition}

experiment:
  task: hallucination
  noise_scheme: Gaussianatoms
  debug: False
  seed: 123
  num_devices: 2
  warm_start: ckpt/se3-fm/hallucination_pdb_IGAattn_GaussUpdate_12D_Gnoise_gtGeoAttn_logdetClamp20_sigmaMin03nm_scaleP_geoInitM4/2026-02-21_22-14-58/last.ckpt
  warm_start_cfg_override: True
  training:
    mask_plddt: True
    bb_atom_scale: 0.1
    trans_scale: 0.1
    translation_loss_weight: 2.0
    t_normalize_clip: 0.9
    rotation_loss_weights: 1.0
    aux_loss_weight: 0.0
    aux_loss_use_bb_loss: True
    aux_loss_use_pair_loss: True
    aux_loss_t_pass: 0.5
    ellipsoid_local_mean_loss_weight: 1.0
    ellipsoid_scaling_loss_weight: 1.0
    # Amino acid prediction loss
    aa_loss_weight: 1.0
    aa_loss_t_threshold: 0.25         # t-gate: only compute aa loss when t > threshold
    # EllipsoidDecoderV2 loss weights
    decoder_loss_weight: 1.0          # atom14 reconstruction weight
    decoder_aa_loss_weight: 0.5       # decoder's own aa prediction weight
    decoder_use_gt_ellipsoid: true    # use GT ellipsoid params for decoder input
    decoder_atom14_t_threshold: 0.5   # higher threshold for atom14 (needs accurate backbone)
    decoder_detach_node_embed: true   # detach node_embed to prevent decoder affecting trunk
    # IGA debug logging (only active when the model returns `iga_debug`, i.e. FlowModelIGA backbone-only path).
    # Set to 1 to log layer-level stats every step (more overhead), or a larger number to reduce logging volume.
    iga_debug_interval: 10
  wandb:
    # For IPA-vs-IGA debugging, encode the backbone here so runs are comparable in W&B.
    # Current code path: FlowModelIGA uses IGA attention + IPA BackboneUpdate (no GaussianUpdateBlock).
    name: ${data.task}_${data.dataset}_IGAattn_GaussUpdate_12D_Gnoise_curriculum_logdetClamp20_sigmaMin03nm_scaleP_geoInitM4
    project: se3-fm
    notes: >
      Curriculum training: warm-start from epoch-15 gtGeoAttn checkpoint.
      iga_geo_attn_mode=mix with gt_prob schedule: 1.0 (0-2k) → linear decay → 0.0 (8k+).
      Gradually removes GT ellipsoid geometry from IGA attention to bridge train-inference gap.
      Step 2 gap test showed 2.38x trans degradation without GT — curriculum should close this.
      Base config unchanged: gaussian_params.corrupt=true, 12D flow matching,
      logdet_min=-20, sigma_min_nm=0.03, 1/sqrt(P) scaling, geo_scale init=-4.0.
  optimizer:
    lr: 0.0001
  trainer:
    overfit_batches: 0
    min_epochs: 1 # prevents early stopping
    max_epochs: 1000
    accelerator: gpu
    log_every_n_steps: 1
    deterministic: False
    strategy: ddp
    check_val_every_n_epoch: 4
    accumulate_grad_batches: 2
    num_sanity_val_steps: 0
    # 正常训练模式 (改成0只跑验证)
    limit_train_batches: 1.0
    limit_val_batches: 1.0
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    save_last: True
    save_top_k: 3
    monitor: valid/non_coil_percent
    mode: max
  # Keep this null. Will be populated at runtime.
  inference_dir: null

model:
  # Sequence prediction head on trunk output
  enable_aa_head: true
  aa_head:
    c_hidden: 256
    num_layers: 3
    dropout: 0.1
  # EllipsoidDecoderV2: two-head (aatype + atom14) from node_embed + ellipsoid
  ellipsoid_decoder:
    enable: true
    version: 2
    c_in: 256               # must match model.ipa.c_s
    d_model: 256
    num_shared_blocks: 2
    num_atom14_blocks: 2
    aatype_embed_dim: 64
    dropout: 0.0
    out_range: 16.0
  iga_geo_attn_mode: mix
  curriculum:
    enable: true
    gt_prob_start: 1.0
    gt_prob_end: 0.0
    warmup_steps: 2000      # gt_prob=1.0 for first 2k steps
    decay_steps: 8000        # linear decay to gt_prob_end by step 8k
