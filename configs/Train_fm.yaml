defaults:
  - _self_
  - datasets
  - model

data:

  # Available tasks: hallucination, inpainting
  task: hallucination

  # Available tasks: pdb, scope
  dataset: pdb

  loader:
    num_workers: 4
    prefetch_factor: 10

  sampler:
    # Setting for 48GB GPUs
    max_batch_size: 10
    max_num_res_squared: 60_000

interpolant:
  min_t: 1e-2

  twisting:
    use: False

  rots:
    corrupt: True
    sample_schedule: exp
    exp_rate: 10

  trans:
    corrupt: True
    batch_ot: True
    sample_schedule: linear
    sample_temp: 1.0
    vpsde_bmin: 0.1
    vpsde_bmax: 20.0
    potential: null
    potential_t_scaling: False
    rog:
      weight: 10.0
      cutoff: 5.0

  gaussian_params:
    # When training/diagnosing IGA attention, keeping ellipsoid params fixed can
    # prevent geometry logits from saturating due to corrupted local_mean/scaling.
    corrupt: True

  sampling:
    num_timesteps: 100
    do_sde: False

  self_condition: ${model.edge_features.self_condition}

experiment:
  task: hallucination
  noise_scheme: Gaussianatoms
  debug: False
  seed: 123
  num_devices: 2
  warm_start: null
  warm_start_cfg_override: True
  training:
    mask_plddt: True
    bb_atom_scale: 0.1
    trans_scale: 0.1
    translation_loss_weight: 2.0
    t_normalize_clip: 0.9
    rotation_loss_weights: 1.0
    aux_loss_weight: 0.0
    aux_loss_use_bb_loss: True
    aux_loss_use_pair_loss: True
    aux_loss_t_pass: 0.5
    ellipsoid_local_mean_loss_weight: 1.0
    ellipsoid_scaling_loss_weight: 1.0
    # IGA debug logging (only active when the model returns `iga_debug`, i.e. FlowModelIGA backbone-only path).
    # Set to 1 to log layer-level stats every step (more overhead), or a larger number to reduce logging volume.
    iga_debug_interval: 10
  wandb:
    # For IPA-vs-IGA debugging, encode the backbone here so runs are comparable in W&B.
    # Current code path: FlowModelIGA uses IGA attention + IPA BackboneUpdate (no GaussianUpdateBlock).
    name: ${data.task}_${data.dataset}_IGAattn_GaussUpdate_12D_Gnoise_gtGeoAttn_logdetClamp20_sigmaMin03nm_scaleP_geoInitM4
    project: se3-fm
    notes: >
      12D run with GT ellipsoid geometry for IGA attention. Corrupted ellipsoid params are
      denoised by GaussianUpdateBlock but IGA attention uses clean GT geometry for stable
      Gaussian overlap. Previous 12D run showed loss regression because corrupted local_mean
      and scaling_log at low t produced meaningless Gaussian overlap distances in attention.
      This fix decouples: iga_rigids_nm (GT ellipsoid + updated backbone) for attention,
      rigids_nm (corrupted ellipsoid) for the update/denoising block.
      Config: gaussian_params.corrupt=true, local_mean + scaling_log loss enabled.
      IGA stability: logdet_min=-20, sigma_min_nm=0.03, 1/sqrt(P) scaling, geo_scale init=-4.0.
  optimizer:
    lr: 0.0001
  trainer:
    overfit_batches: 0
    min_epochs: 1 # prevents early stopping
    max_epochs: 1000
    accelerator: gpu
    log_every_n_steps: 1
    deterministic: False
    strategy: ddp
    check_val_every_n_epoch: 4
    accumulate_grad_batches: 2
    num_sanity_val_steps: 0
    # 正常训练模式 (改成0只跑验证)
    limit_train_batches: 1.0
    limit_val_batches: 1.0
  checkpointer:
    dirpath: ckpt/${experiment.wandb.project}/${experiment.wandb.name}/${now:%Y-%m-%d}_${now:%H-%M-%S}
    save_last: True
    save_top_k: 3
    monitor: valid/non_coil_percent
    mode: max
  # Keep this null. Will be populated at runtime.
  inference_dir: null
